{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "findMax.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pn4hMO267RgL",
        "outputId": "d5465eb1-9c95-468f-b006-fa32f809e34b"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "import nltk\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "from transformers import (GPT2LMHeadModel, GPT2Tokenizer)\n",
        "from matplotlib import pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 8.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 43.4MB/s \n",
            "\u001b[?25h  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVLtOOPU7Kxx"
      },
      "source": [
        "class AbstractLanguageChecker():\n",
        "    \"\"\"\n",
        "    Abstract Class that defines the Backend API of GLTR.\n",
        "\n",
        "    To extend the GLTR interface, you need to inherit this and\n",
        "    fill in the defined functions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        In the subclass, you need to load all necessary components\n",
        "        for the other functions.\n",
        "        Typically, this will comprise a tokenizer and a model.\n",
        "        '''\n",
        "        self.device = torch.device(\n",
        "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def check_probabilities(self, in_text, topk=40):\n",
        "        '''\n",
        "        Function that GLTR interacts with to check the probabilities of words\n",
        "\n",
        "        Params:\n",
        "        - in_text: str -- The text that you want to check\n",
        "        - topk: int -- Your desired truncation of the head of the distribution\n",
        "\n",
        "        Output:\n",
        "        - payload: dict -- The wrapper for results in this function, described below\n",
        "\n",
        "        Payload values\n",
        "        ==============\n",
        "        bpe_strings: list of str -- Each individual token in the text\n",
        "        real_topk: list of tuples -- (ranking, prob) of each token\n",
        "        pred_topk: list of list of tuple -- (word, prob) for all topk\n",
        "        '''\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def postprocess(self, token):\n",
        "        \"\"\"\n",
        "        clean up the tokens from any special chars and encode\n",
        "        leading space by UTF-8 code '\\u0120', linebreak with UTF-8 code 266 '\\u010A'\n",
        "        :param token:  str -- raw token text\n",
        "        :return: str -- cleaned and re-encoded token text\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "def top_k_logits(logits, k):\n",
        "    '''\n",
        "    Filters logits to only the top k choices\n",
        "    from https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/run_gpt2.py\n",
        "    '''\n",
        "    if k == 0:\n",
        "        return logits\n",
        "    values, _ = torch.topk(logits, k)\n",
        "    min_values = values[:, -1]\n",
        "    return torch.where(logits < min_values,\n",
        "                       torch.ones_like(logits, dtype=logits.dtype) * -1e10,\n",
        "                       logits)\n",
        "\n",
        "\n",
        "\n",
        "class LM(AbstractLanguageChecker):\n",
        "    def __init__(self, model_name_or_path=\"gpt2\"):\n",
        "        super(LM, self).__init__()\n",
        "        self.enc = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_name_or_path)\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        self.start_token = '<|endoftext|>'\n",
        "        #self.start_token = 'My father'\n",
        "        print(\"Loaded GPT-2 model!\")\n",
        "\n",
        "    def check_probabilities(self, in_text, topk=40):\n",
        "        # Process input\n",
        "        start_t = torch.full((1, 1),\n",
        "                             self.enc.encoder[self.start_token],\n",
        "                             device=self.device,\n",
        "                             dtype=torch.long)\n",
        "        context = self.enc.encode(in_text)\n",
        "        context = torch.tensor(context,\n",
        "                               device=self.device,\n",
        "                               dtype=torch.long).unsqueeze(0)\n",
        "        context = torch.cat([start_t, context], dim=1)\n",
        "        # Forward through the model\n",
        "        logits, _ = self.model(context)\n",
        "\n",
        "        # construct target and pred\n",
        "        yhat = torch.softmax(logits[0, :-1], dim=-1)\n",
        "        y = context[0, 1:]\n",
        "        # Sort the predictions for each timestep\n",
        "        sorted_preds = np.argsort(-yhat.data.cpu().numpy())\n",
        "        # [(pos, prob), ...]\n",
        "        real_topk_pos = list(\n",
        "            [int(np.where(sorted_preds[i] == y[i].item())[0][0])\n",
        "             for i in range(y.shape[0])])\n",
        "        real_topk_probs = yhat[np.arange(\n",
        "            0, y.shape[0], 1), y].data.cpu().numpy().tolist()\n",
        "        real_topk_probs = list(map(lambda x: round(x, 5), real_topk_probs))\n",
        "\n",
        "        real_topk = list(zip(real_topk_pos, real_topk_probs))\n",
        "        # [str, str, ...]\n",
        "        bpe_strings = [self.enc.decoder[s.item()] for s in context[0]]\n",
        "\n",
        "        bpe_strings = [self.postprocess(s) for s in bpe_strings]\n",
        "\n",
        "        # [[(pos, prob), ...], [(pos, prob), ..], ...]\n",
        "        pred_topk = [\n",
        "            list(zip([self.enc.decoder[p] for p in sorted_preds[i][:topk]],\n",
        "                     list(map(lambda x: round(x, 5),\n",
        "                              yhat[i][sorted_preds[i][\n",
        "                                      :topk]].data.cpu().numpy().tolist()))))\n",
        "            for i in range(y.shape[0])]\n",
        "\n",
        "        pred_topk = [[(self.postprocess(t[0]), t[1]) for t in pred] for pred in pred_topk]\n",
        "        payload = {'bpe_strings': bpe_strings,\n",
        "                   'real_topk': real_topk,\n",
        "                   'pred_topk': pred_topk}\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        return payload\n",
        "\n",
        "    def sample_unconditional(self, length=100, topk=5, temperature=1.0):\n",
        "        '''\n",
        "        Sample `length` words from the model.\n",
        "        Code strongly inspired by\n",
        "        https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/run_gpt2.py\n",
        "\n",
        "        '''\n",
        "        \"\"\"context = torch.full((1, 1),\n",
        "                             #self.enc.encoder[self.start_token],\n",
        "                             self.enc('I enjoy walking with my cute dog')['input_ids'],\n",
        "                             #self.enc.encode('I enjoy walking with my cute dog', return_tensors='pt'),\n",
        "                             device=self.device,\n",
        "                             dtype=torch.long)\n",
        "\n",
        "        \"\"\"\n",
        "        context=torch.Tensor(GPT2Tokenizer.from_pretrained(\"gpt2\")(\"hello worlds\")['input_ids'], \n",
        "                             device=self.device,\n",
        "                             dtype=torch.long)\n",
        "        prev = context\n",
        "        output = context\n",
        "        past = None\n",
        "        # Forward through the model\n",
        "        output_list1=[]\n",
        "        output_list2=[]\n",
        "        with torch.no_grad():\n",
        "            for i in range(length):\n",
        "                logits, past = self.model(prev, past=past)\n",
        "                logits = logits[:, -1, :] / temperature\n",
        "                # Filter predictions to topk and softmax\n",
        "                probs = torch.softmax(top_k_logits(logits, k=topk),\n",
        "                                      dim=-1)\n",
        "                # Sample\n",
        "                sorted=torch.argsort(probs)\n",
        "               # print(sorted.shape)\n",
        "                prev = torch.multinomial(probs, num_samples=1)\n",
        "                print(prev)\n",
        "                print(torch.sum(probs))\n",
        "                best=sorted[0]\n",
        "                print(best)\n",
        "                print(best.reshape(prev.shape))\n",
        "                # Construct output\n",
        "                output = torch.cat((output, prev), dim=1)\n",
        "              #  print(sorted[0, 0])\n",
        "                #output_list1.append(sorted[0, 0])\n",
        "                #output_list2.append(sorted[0, 1])\n",
        "        output_text = self.enc.decode(output[0].tolist())\n",
        "        #output_text = self.enc.decode(output_list1)\n",
        "        return output_text \n",
        "    \n",
        "    def sample_unconditional_old(self, length=100, topk=5, temperature=1.0):\n",
        "        '''\n",
        "        Sample `length` words from the model.\n",
        "        Code strongly inspired by\n",
        "        https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/run_gpt2.py\n",
        "\n",
        "        '''\n",
        "        context = torch.full((1, 1),\n",
        "                             self.enc.encoder[self.start_token],\n",
        "                             device=self.device,\n",
        "                             dtype=torch.long)\n",
        "        prev = context\n",
        "        output = context\n",
        "        past = None\n",
        "        # Forward through the model\n",
        "        output_list1=[]\n",
        "        output_list2=[]\n",
        "        with torch.no_grad():\n",
        "            for i in range(length):\n",
        "                logits, past = self.model(prev, past=past)\n",
        "                logits = logits[:, -1, :] / temperature\n",
        "                # Filter predictions to topk and softmax\n",
        "                probs = torch.softmax(top_k_logits(logits, k=topk),\n",
        "                                      dim=-1)\n",
        "                # Sample\n",
        "                sorted=torch.argsort(probs)\n",
        "          #      print(sorted.shape)\n",
        "                prev = torch.multinomial(probs, num_samples=1)\n",
        "                # Construct output\n",
        "                output = torch.cat((output, prev), dim=1)\n",
        "         #       print(sorted[0, 0])\n",
        "                output_list1.append(sorted[0, 0])\n",
        "                output_list2.append(sorted[0, 1])\n",
        "        output_text = self.enc.decode(output[0].tolist())\n",
        "        #output_text = self.enc.decode(output_list1)\n",
        "        return output_text\n",
        "\n",
        "    def postprocess(self, token):\n",
        "        with_space = False\n",
        "        with_break = False\n",
        "        if token.startswith('Ġ'):\n",
        "            with_space = True\n",
        "            token = token[1:]\n",
        "            # print(token)\n",
        "        elif token.startswith('â'):\n",
        "            token = ' '\n",
        "        elif token.startswith('Ċ'):\n",
        "            token = ' '\n",
        "            with_break = True\n",
        "\n",
        "        token = '-' if token.startswith('â') else token\n",
        "        token = '“' if token.startswith('ľ') else token\n",
        "        token = '”' if token.startswith('Ŀ') else token\n",
        "        token = \"'\" if token.startswith('Ļ') else token\n",
        "\n",
        "        if with_space:\n",
        "            token = '\\u0120' + token\n",
        "        if with_break:\n",
        "            token = '\\u010A' + token\n",
        "\n",
        "        return token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yf1ifT9M7PMl"
      },
      "source": [
        "def plot_text(vals, what, name):\n",
        "    if what==\"prob\":\n",
        "        ourvals = vals[0]\n",
        "        x = list(range(1,len(ourvals)+1))\n",
        "        y = ourvals\n",
        "        plt.plot(x, y, color='orange')\n",
        "        plt.ylim(0,1)\n",
        "        plt.savefig(name + \".png\")\n",
        "        # plt.show()\n",
        "    elif what==\"rank\":\n",
        "        ourvals = vals[1]\n",
        "        x = list(range(1, len(ourvals) + 1))\n",
        "        y = ourvals\n",
        "        plt.plot(x, y, color='orange')\n",
        "        plt.ylim(-1000, 50000)\n",
        "        plt.savefig(name + \".png\")\n",
        "        # plt.show()\n",
        "def main_code(raw_text):\n",
        "    lm = LM()\n",
        "    start = time.time()\n",
        "    payload = lm.check_probabilities(raw_text, topk=5)\n",
        "    # print(payload[\"pred_topk\"])\n",
        "    real_topK = payload[\"real_topk\"]\n",
        "    ranks = [i[0] for i in real_topK]\n",
        "    preds = [i[1] for i in real_topK]\n",
        "    plot_text([preds, ranks], 'rank', \"rank_\")\n",
        "    end = time.time()\n",
        "    print(\"{:.2f} Seconds for a check with GPT-2\".format(end - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GdpkEKL7lgQ"
      },
      "source": [
        "text=\"This is a tree. I love it as it is green and full. \""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve4rnLz37ryW"
      },
      "source": [
        "#main_code(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "BJVmx5CM7s9e",
        "outputId": "8a85b372-756d-45fc-bb71-fd9bcc284c52"
      },
      "source": [
        "lm = LM()\n",
        "res=lm.sample_unconditional(topk=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded GPT-2 model!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-115-4945ce97ea03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_unconditional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-111-dd7d31eb2334>\u001b[0m in \u001b[0;36msample_unconditional\u001b[0;34m(self, length, topk, temperature)\u001b[0m\n\u001b[1;32m    138\u001b[0m         context=torch.Tensor(GPT2Tokenizer.from_pretrained(\"gpt2\")(\"hello worlds\")['input_ids'], \n\u001b[1;32m    139\u001b[0m                              \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                              dtype=torch.long)\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: new() received an invalid combination of arguments - got (list, dtype=torch.dtype, device=torch.device), but expected one of:\n * (*, torch.device device)\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, *, torch.device device)\n      didn't match because some of the keywords were incorrect: dtype\n * (object data, *, torch.device device)\n      didn't match because some of the keywords were incorrect: dtype\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F0rKoJr9P3R"
      },
      "source": [
        "print(res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6v9eTDr939h"
      },
      "source": [
        "from transformers import GPT2Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ54TwTGd1T6",
        "outputId": "507c812d-5a0d-4426-9397-9d0fc7efa0a8"
      },
      "source": [
        "GPT2Tokenizer.from_pretrained(\"gpt2\")(\"hello worlds\")['input_ids']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[31373, 11621]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "DFjEc7Zxd5bA",
        "outputId": "46a5ad6d-f1f4-4805-e8e9-97bd8f5ef140"
      },
      "source": [
        "context = torch.full(GPT2Tokenizer.from_pretrained(\"gpt2\")(\"hello worlds\")['input_ids'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-102-f621bad043ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gpt2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hello worlds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: full() received an invalid combination of arguments - got (list), but expected one of:\n * (tuple of ints size, Number fill_value, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, Number fill_value, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXO9uOk3eETV"
      },
      "source": [
        "a=GPT2Tokenizer.from_pretrained(\"gpt2\")(\"hello worlds\")['input_ids']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtQZ4fPbeOlk",
        "outputId": "0f948adf-0a34-4f28-a201-c87cdd136e56"
      },
      "source": [
        "torch.Tensor(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([31373., 11621.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtgIV9sBelSM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
