{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DemoStego.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIBQWSvYVmjP"
      },
      "source": [
        "from wrappedCode.createModel import *\n",
        "from wrappedCode.encryptionWrapped import *\n",
        "from wrappedCode.evaluationModelHelpers import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6r0W-gQVxq9"
      },
      "source": [
        "##\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lB1oSRwuW4IJ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2abLsCiVtJc"
      },
      "source": [
        "## Model selection\n",
        "\n",
        "As different models were used for the example, these models need to be chosen as part of the model selection. For GPT2 models, it can be chosen between:\n",
        "\n",
        "\n",
        "*   \"gpt2-small\"\n",
        "*   \"gpt2-medium\"\n",
        "*   \"gpt2-large\"\n",
        "*   \"gpt2-xl\"\n",
        "\n",
        "Additionally, BERT and RoBERTa can also be selected.  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rr0hq4eqWGiG"
      },
      "source": [
        "mod, tok=buildModelGPT(modelType=\"gpt2-large\") # make nice wrapper for this!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Kz3jSLDVwh_"
      },
      "source": [
        "## Encryption of the secret text\n",
        "\n",
        "Depending on the choice, the encryption can be conducted with complete sentences or incomplete sentences. For this example, the start of Adele's \"Hello\" is encrypted. \n",
        "As part of the encryption it can be decided, whether the last sentence should be completed. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaIYolE7dCI0"
      },
      "source": [
        "startOfText=\"This year's Shakespeare Festival\"\n",
        "precondSec=\"Secret: \"\n",
        "secret=\"\"\"Hello, it's me\n",
        "I was wondering if after all these years you'd like to meet\n",
        "To go over everything\n",
        "They say that time's supposed to heal ya\n",
        "But I ain't done much healing\n",
        "Hello, can you hear me?\n",
        "I'm in California dreaming about who we used to be\n",
        "When we were younger and free\n",
        "I've forgotten how it felt before the world fell at our feet\n",
        "There's such a difference between us\n",
        "And a million miles\n",
        "Hello from the other side\n",
        "I must've called a thousand times\n",
        "To tell you I'm sorry for everything that I've done\n",
        "But when I call, you never seem to be home\"\"\"\n",
        "sentenceComplete=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0USeAmZWWGMR"
      },
      "source": [
        "outText, outInd=encryptMessage(mod, tok, secret, precondSec, startOfText, completeSentence=sentenceComplete)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee4PUaNrV4A6"
      },
      "source": [
        "## Decryption of the cover text\n",
        "\n",
        "For the decryption, the receiver needs to know the preconditioning of the secret and the start of the text. Given this and knowing, whether sentence completion was activated, the text can be recovered correctly. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leKChzzcdYSU"
      },
      "source": [
        "print(getTextFromText(mod, tok, outText, precondSec, startOfText, True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B8BYrkbWFjE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJSo8NynWHub"
      },
      "source": [
        "## Evaluation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HijsrydLWwfC"
      },
      "source": [
        "### Smoothness of the generated text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkraBA3gXYIt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0yE4IOuWyo1"
      },
      "source": [
        "def plot_ranks_bert(mod, tok, precondSec, secret):\n",
        "  ranks = []\n",
        "  x = range(len(tok.encode(secret, add_special_tokens=False)))\n",
        "  ranks = get_ranks(mod, tok, precondSec, secret)\n",
        "  plt.plot(x, ranks, color='orange')\n",
        "  plt.ylim(-1000, 50000)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_ranks_gpt2(model_gpt, tok_gpt, precondSec, secret):\n",
        "  x = range(len(tok_gpt.encode(secret)))\n",
        "  ranks=getSecretRanks(model_gpt, tok_gpt, secret, precondSec)\n",
        "  plt.plot(x, ranks, color='orange')\n",
        "  plt.ylim(-1000, 50000)\n",
        "  plt.show()\n",
        "\n",
        "if GPT2_:\n",
        "  plot_ranks_gpt2(model_gpt, tok_gpt, precondSec, secret_text)\n",
        "elif BERT_:  \n",
        "  plot_ranks_bert(mod, tok, precondSec, secret_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKzfsIrpWJ90"
      },
      "source": [
        "### Perplexity score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0A2NYNosWOyL"
      },
      "source": [
        "def get_perplex_score(cover_text, model, tokenizer, startingSecret=\". \"):\n",
        "    probas = []\n",
        "    token_secret = tokenizer.encode(cover_text)\n",
        "    token_start = tokenizer.encode(startingSecret)\n",
        "    # Convert indexed tokens in a PyTorch tensor\n",
        "    tokens_tensor = torch.tensor([token_start])\n",
        "    m = nn.Softmax(dim=0)\n",
        "  # If you have a GPU, put everything on cuda\n",
        "    tokens_tensor = tokens_tensor.to('cuda')\n",
        "    model.to('cuda')\n",
        "    pred = []\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensor)\n",
        "        predictions = outputs[0]\n",
        "        tab = m(predictions[:, -1, :][0])\n",
        "        pred.append(tab[token_secret[0]].item())\n",
        "        for i in range(1, len(token_secret)):\n",
        "            tokens_tensor = torch.cat((tokens_tensor.to('cpu').view(-1), torch.Tensor([token_secret[i]])), dim=-1).view(1, -1)\n",
        "            outputs = model(tokens_tensor.type(torch.long).to(\"cuda\"))\n",
        "            predictions = outputs[0]\n",
        "            tab = m(predictions[:, -1, :][0])\n",
        "            pred.append(tab[token_secret[i]].item())\n",
        "            \n",
        "    s = 0\n",
        "    for p in pred:\n",
        "        s += np.log2(p)\n",
        "    score = 2**((-1/len(pred))*s)\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}