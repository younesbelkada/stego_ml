{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IIBQWSvYVmjP"
   },
   "outputs": [],
   "source": [
    "from wrappedCode.createModel import *\n",
    "from wrappedCode.encryptionWrapped import *\n",
    "from wrappedCode.decryptionWrapped import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2abLsCiVtJc"
   },
   "source": [
    "## Model selection\n",
    "\n",
    "As different models were used for the example, these models need to be chosen as part of the model selection. For GPT2 models, it can be chosen between:\n",
    "\n",
    "\n",
    "*   \"gpt2-small\"\n",
    "*   \"gpt2-medium\"\n",
    "*   \"gpt2-large\"\n",
    "*   \"gpt2-xl\"\n",
    "\n",
    "Additionally, BERT and RoBERTa can also be selected.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rr0hq4eqWGiG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "mod, tok= buildModel(\"bert-base-uncased\") # make nice wrapper for this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Kz3jSLDVwh_"
   },
   "source": [
    "## Encryption of the secret text\n",
    "\n",
    "Depending on the choice, the encryption can be conducted with complete sentences or incomplete sentences. For this example, the start of Adele's \"Hello\" is encrypted. \n",
    "As part of the encryption it can be decided, whether the last sentence should be completed. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZaIYolE7dCI0"
   },
   "outputs": [],
   "source": [
    "startOfText=\"This year's Shakespeare Festival\"\n",
    "precondSec=\"Secret: \"\n",
    "secret=\"\"\"Hello, it's me\n",
    "I was wondering if after all these years you'd like to meet\n",
    "To go over everything\n",
    "They say that time's supposed to heal ya\n",
    "But I ain't done much healing\n",
    "Hello, can you hear me?\n",
    "I'm in California dreaming about who we used to be\n",
    "When we were younger and free\n",
    "I've forgotten how it felt before the world fell at our feet\n",
    "There's such a difference between us\n",
    "And a million miles\n",
    "Hello from the other side\n",
    "I must've called a thousand times\n",
    "To tell you I'm sorry for everything that I've done\n",
    "But when I call, you never seem to be home\"\"\"\n",
    "sentenceComplete=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0USeAmZWWGMR"
   },
   "outputs": [],
   "source": [
    "outText, outInd=encryptMessage(mod, tok, secret, precondSec, startOfText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cover text : this year's shakespeare festivalurities true magazine tr small - all pre constant jen {nnantaalllayrahlahorate backstage for dear far great otherwise o is element all or bound love torch mentor an afar learnt wisdom and truth faith worries ridges te pension'not co an non non non pal buffer safely ni aa when took i i never did aye &. nay \" ; nline as / ran > en off demiseours if parentheses corner to to r. l or ex = was middle me jennings = # \" waist lean tip to hem fruit tale substantial ~. min of % * * n waist waist the, was remained which un thoughts remained de often it at free many intoit to [CLS]. [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(\"Cover text : {}\".format(outText))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ee4PUaNrV4A6"
   },
   "source": [
    "## Decryption of the cover text\n",
    "\n",
    "For the decryption, the receiver needs to know the preconditioning of the secret and the start of the text. Given this and knowing, whether sentence completion was activated, the text can be recovered correctly. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = decryptMessage(mod, tok, outText, precondSec, startOfText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrypted text : secret : hello, it's me i was wondering if after before arriving valle? do depends de then ago final then take hit watch note if rest sal like is myself once raced tasted back cloth revolution of as? some plus total subjecthwa and note long on remained or or those stir wentston would t store in in too it time another was sure turned for removing art back way mom changed mother'danced they various followed matches play play the play all over by of for killed released devotion remorse,d wonder just midnight nightd luce an serpent disappeared as above still played the the was loop bit is'which with of together atoms. begin hear listen wonder think form your coming\n"
     ]
    }
   ],
   "source": [
    "print(\"Decrypted text : {}\".format(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJSo8NynWHub"
   },
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HijsrydLWwfC"
   },
   "source": [
    "### Smoothness of the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GkraBA3gXYIt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P0yE4IOuWyo1"
   },
   "outputs": [],
   "source": [
    "def plot_ranks_bert(mod, tok, precondSec, secret):\n",
    "  ranks = []\n",
    "  x = range(len(tok.encode(secret, add_special_tokens=False)))\n",
    "  ranks = get_ranks(mod, tok, precondSec, secret)\n",
    "  plt.plot(x, ranks, color='orange')\n",
    "  plt.ylim(-1000, 50000)\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_ranks_gpt2(model_gpt, tok_gpt, precondSec, secret):\n",
    "  x = range(len(tok_gpt.encode(secret)))\n",
    "  ranks=getSecretRanks(model_gpt, tok_gpt, secret, precondSec)\n",
    "  plt.plot(x, ranks, color='orange')\n",
    "  plt.ylim(-1000, 50000)\n",
    "  plt.show()\n",
    "\n",
    "if GPT2_:\n",
    "  plot_ranks_gpt2(model_gpt, tok_gpt, precondSec, secret_text)\n",
    "elif BERT_:  \n",
    "  plot_ranks_bert(mod, tok, precondSec, secret_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKzfsIrpWJ90"
   },
   "source": [
    "### Perplexity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0A2NYNosWOyL"
   },
   "outputs": [],
   "source": [
    "def get_perplex_score(cover_text, model, tokenizer, startingSecret=\". \"):\n",
    "    probas = []\n",
    "    token_secret = tokenizer.encode(cover_text)\n",
    "    token_start = tokenizer.encode(startingSecret)\n",
    "    # Convert indexed tokens in a PyTorch tensor\n",
    "    tokens_tensor = torch.tensor([token_start])\n",
    "    m = nn.Softmax(dim=0)\n",
    "  # If you have a GPU, put everything on cuda\n",
    "    tokens_tensor = tokens_tensor.to('cuda')\n",
    "    model.to('cuda')\n",
    "    pred = []\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor)\n",
    "        predictions = outputs[0]\n",
    "        tab = m(predictions[:, -1, :][0])\n",
    "        pred.append(tab[token_secret[0]].item())\n",
    "        for i in range(1, len(token_secret)):\n",
    "            tokens_tensor = torch.cat((tokens_tensor.to('cpu').view(-1), torch.Tensor([token_secret[i]])), dim=-1).view(1, -1)\n",
    "            outputs = model(tokens_tensor.type(torch.long).to(\"cuda\"))\n",
    "            predictions = outputs[0]\n",
    "            tab = m(predictions[:, -1, :][0])\n",
    "            pred.append(tab[token_secret[i]].item())\n",
    "            \n",
    "    s = 0\n",
    "    for p in pred:\n",
    "        s += np.log2(p)\n",
    "    score = 2**((-1/len(pred))*s)\n",
    "    return score"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DemoStego.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "stego",
   "language": "python",
   "name": "stego"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
